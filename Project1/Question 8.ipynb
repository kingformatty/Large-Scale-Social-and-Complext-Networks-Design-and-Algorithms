{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implement new dataset\n",
    "categories_computer_sys_ibm=['comp.sys.ibm.pc.hardware']\n",
    "categories_computer_sys_mac=['comp.sys.mac.hardware']\n",
    "categories_misc_forsale=['misc.forsale']\n",
    "categories_soc_re_chris=['soc.religion.christian']\n",
    "computer_sys_ibm_train=datasets.fetch_20newsgroups(subset = 'train', categories = categories_computer_sys_ibm, shuffle = True, random_state = None).data\n",
    "computer_sys_ibm_test=datasets.fetch_20newsgroups(subset = 'test', categories = categories_computer_sys_ibm, shuffle = True, random_state = None).data\n",
    "computer_sys_mac_train=datasets.fetch_20newsgroups(subset = 'train', categories = categories_computer_sys_mac, shuffle = True, random_state = None).data\n",
    "computer_sys_mac_test=datasets.fetch_20newsgroups(subset = 'test', categories = categories_computer_sys_mac, shuffle = True, random_state = None).data\n",
    "misc_forsale_train=datasets.fetch_20newsgroups(subset = 'train', categories = categories_misc_forsale, shuffle = True, random_state = None).data\n",
    "misc_forsale_test=datasets.fetch_20newsgroups(subset = 'test', categories = categories_misc_forsale, shuffle = True, random_state = None).data\n",
    "soc_re_chris_train=datasets.fetch_20newsgroups(subset = 'train', categories = categories_soc_re_chris, shuffle = True, random_state = None).data\n",
    "soc_re_chris_test=datasets.fetch_20newsgroups(subset = 'test', categories = categories_soc_re_chris, shuffle = True, random_state = None).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3917,)\n"
     ]
    }
   ],
   "source": [
    "X_train=computer_sys_ibm_train+computer_sys_mac_train+misc_forsale_train+soc_re_chris_train\n",
    "X_test=computer_sys_ibm_test+computer_sys_mac_test+misc_forsale_test+soc_re_chris_test\n",
    "X_overall=X_train+X_test\n",
    "Y_train=[0]*len(computer_sys_ibm_train)+[1]*len(computer_sys_mac_train)+[2]*len(misc_forsale_train)+[3]*len(soc_re_chris_train)\n",
    "Y_test=[0]*len(computer_sys_ibm_test)+[1]*len(computer_sys_mac_test)+[2]*len(misc_forsale_test)+[3]*len(soc_re_chris_test)\n",
    "print(np.shape(X_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "#Define lemmatizer pos_tag to deal with adj, verb, noun and adv separately\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Preprocessing: exclude some symbols and numbers in each sentence\n",
    "exclude_sign=\"~#$%^&*(){}[]<>|+=1234567890\"\n",
    "replace_sign=\"@,.?!-;\"\n",
    "def preprocessing(data):\n",
    "    processed_sentence=[]\n",
    "    for i in range(len(data)):\n",
    "        sentence=data[i]\n",
    "        for c in exclude_sign:\n",
    "            sentence=sentence.replace(c,\"\")\n",
    "        for c in replace_sign:\n",
    "            sentence=sentence.replace(c,\" \")\n",
    "        processed_token=[lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
    "        processed_sentence.append(\" \".join(processed_token))\n",
    "    return processed_sentence\n",
    "\n",
    "#operation of lemmaization\n",
    "processed_data=preprocessing(X_overall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3917, 50)\n",
      "(2352, 50)\n",
      "(1565, 50)\n"
     ]
    }
   ],
   "source": [
    "#Vectorization\n",
    "vectorizer=CountVectorizer(stop_words='english',min_df=3)\n",
    "data_vec=vectorizer.fit_transform(processed_data)\n",
    "\n",
    "#TdIdf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "TdT=TfidfTransformer()\n",
    "data_vec_ti=TdT.fit_transform(data_vec)\n",
    "\n",
    "#LSI\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "transformer=TruncatedSVD(n_components=50)\n",
    "data_svd=transformer.fit_transform(data_vec_ti)\n",
    "print(np.shape(data_svd))\n",
    "\n",
    "#Divide train rows and test rows\n",
    "X_train_tf=data_svd[0:len(X_train)]\n",
    "print(np.shape(X_train_tf))\n",
    "X_test_tf=data_svd[len(X_train):]\n",
    "print(np.shape(X_test_tf))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "def GaussiainNB_ovo (X_train,Y_train, X_test, Y_test):\n",
    "    ono_classifier=OneVsOneClassifier(GaussianNB())\n",
    "    ono_classifier.fit(X_train, Y_train)\n",
    "    predict_target=ono_classifier.predict(X_test)\n",
    "    confusion_matrix=metrics.confusion_matrix(Y_test, predict_target)\n",
    "    accuracy=metrics.accuracy_score(Y_test, predict_target)\n",
    "    recall=metrics.recall_score(Y_test, predict_target,average = None)\n",
    "    precision=metrics.precision_score(Y_test, predict_target,average = None)\n",
    "    f1_score=metrics.f1_score(Y_test, predict_target,average = None)\n",
    "    print('Naive Bayes---One vs One \\n')\n",
    "    print(confusion_matrix)\n",
    "    print(\"accuracy: {}\\nrecall: {}\\nprecision: {}\\nf1: {}\".format(accuracy, recall, precision, f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes---One vs One \n",
      "\n",
      "[[247  58  70  17]\n",
      " [ 72 248  57   8]\n",
      " [ 56  40 282  12]\n",
      " [  2   2   3 391]]\n",
      "accuracy: 0.7463258785942491\n",
      "recall: [0.63010204 0.64415584 0.72307692 0.98241206]\n",
      "precision: [0.65517241 0.71264368 0.68446602 0.9135514 ]\n",
      "f1: [0.64239272 0.67667121 0.7032419  0.94673123]\n"
     ]
    }
   ],
   "source": [
    "GaussiainNB_ovo(X_train_tf,Y_train, X_test_tf, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs Rest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "def GaussiainNB_ovr (X_train,Y_train, X_test, Y_test):\n",
    "    onr_classifier=OneVsRestClassifier(GaussianNB())\n",
    "    onr_classifier.fit(X_train, Y_train)\n",
    "    predict_target=onr_classifier.predict(X_test)\n",
    "    confusion_matrix=metrics.confusion_matrix(Y_test, predict_target)\n",
    "    accuracy=metrics.accuracy_score(Y_test, predict_target)\n",
    "    recall=metrics.recall_score(Y_test, predict_target,average = None)\n",
    "    precision=metrics.precision_score(Y_test, predict_target,average = None)\n",
    "    f1_score=metrics.f1_score(Y_test, predict_target,average = None)\n",
    "    print('Naive Bayes---One vs Rest \\n')\n",
    "    print(confusion_matrix)\n",
    "    print(\"accuracy: {}\\nrecall: {}\\nprecision: {}\\nf1: {}\".format(accuracy, recall, precision, f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes---One vs Rest \n",
      "\n",
      "[[246  62  74  10]\n",
      " [ 66 250  62   7]\n",
      " [ 55  40 285  10]\n",
      " [  0   2   5 391]]\n",
      "accuracy: 0.7488817891373802\n",
      "recall: [0.62755102 0.64935065 0.73076923 0.98241206]\n",
      "precision: [0.67029973 0.70621469 0.66901408 0.9354067 ]\n",
      "f1: [0.64822134 0.67658999 0.69852941 0.95833333]\n"
     ]
    }
   ],
   "source": [
    "GaussiainNB_ovr(X_train_tf,Y_train, X_test_tf, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is: \n",
      "[[247  58  70  17]\n",
      " [ 72 248  57   8]\n",
      " [ 56  40 282  12]\n",
      " [  2   2   3 391]]\n",
      "Accuracy: 0.7463258785942491\n",
      "Recall: [0.63010204 0.64415584 0.72307692 0.98241206]\n",
      "Precision: [0.65517241 0.71264368 0.68446602 0.9135514 ]\n",
      "f1: [0.64239272 0.67667121 0.7032419  0.94673123]\n"
     ]
    }
   ],
   "source": [
    "#predict_target= naive_Gaussian_model(X_train_tf, Y_train, X_test_tf, Y_test)\n",
    "#evaluation(predict_target, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def SVM_ovo (X_train,Y_train, X_test, Y_test):\n",
    "    ono_classifier=OneVsOneClassifier(svm.SVC(C=1000,gamma='auto'))\n",
    "    ono_classifier.fit(X_train, Y_train)\n",
    "    predict_target=ono_classifier.predict(X_test)\n",
    "    confusion_matrix=metrics.confusion_matrix(Y_test, predict_target)\n",
    "    accuracy=metrics.accuracy_score(Y_test, predict_target)\n",
    "    recall=metrics.recall_score(Y_test, predict_target,average = None)\n",
    "    precision=metrics.precision_score(Y_test, predict_target,average = None)\n",
    "    f1_score=metrics.f1_score(Y_test, predict_target,average = None)\n",
    "    print('SVM---One vs One \\n')\n",
    "    print(confusion_matrix)\n",
    "    print(\"accuracy: {}\\nrecall: {}\\nprecision: {}\\nf1: {}\".format(accuracy, recall, precision, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM---One vs One \n",
      "\n",
      "[[331  35  26   0]\n",
      " [ 33 331  21   0]\n",
      " [ 30  21 336   3]\n",
      " [  3   0   2 393]]\n",
      "accuracy: 0.888817891373802\n",
      "recall: [0.84438776 0.85974026 0.86153846 0.98743719]\n",
      "precision: [0.83375315 0.85529716 0.87272727 0.99242424]\n",
      "f1: [0.83903676 0.85751295 0.86709677 0.98992443]\n"
     ]
    }
   ],
   "source": [
    "SVM_ovo(X_train_tf,Y_train, X_test_tf, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs Rest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM_ovr (X_train,Y_train, X_test, Y_test):\n",
    "    onr_classifier=OneVsRestClassifier(svm.SVC(C=1000,gamma='auto'))\n",
    "    onr_classifier.fit(X_train, Y_train)\n",
    "    predict_target=onr_classifier.predict(X_test)\n",
    "    confusion_matrix=metrics.confusion_matrix(Y_test, predict_target)\n",
    "    accuracy=metrics.accuracy_score(Y_test, predict_target)\n",
    "    recall=metrics.recall_score(Y_test, predict_target,average = None)\n",
    "    precision=metrics.precision_score(Y_test, predict_target,average = None)\n",
    "    f1_score=metrics.f1_score(Y_test, predict_target,average = None)\n",
    "    print('SVM---One vs Rest \\n')\n",
    "    print(confusion_matrix)\n",
    "    print(\"accuracy: {}\\nrecall: {}\\nprecision: {}\\nf1: {}\".format(accuracy, recall, precision, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM---One vs Rest \n",
      "\n",
      "[[333  37  20   2]\n",
      " [ 36 329  20   0]\n",
      " [ 26  23 337   4]\n",
      " [  5   1   0 392]]\n",
      "accuracy: 0.888817891373802\n",
      "recall: [0.8494898  0.85454545 0.86410256 0.98492462]\n",
      "precision: [0.8325     0.84358974 0.8938992  0.98492462]\n",
      "f1: [0.84090909 0.84903226 0.87874837 0.98492462]\n"
     ]
    }
   ],
   "source": [
    "SVM_ovr(X_train_tf,Y_train, X_test_tf, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
