{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "import pytz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def function to read and combine data together\n",
    "def readfile_aggregate(file_list):\n",
    "    time_list=[]\n",
    "    num_followers=[]\n",
    "    num_retweets=[]\n",
    "    num_replies=[]\n",
    "    num_mentions=[]\n",
    "    mentions_ratio=[]\n",
    "    num_urls=[]\n",
    "    urls_ratio=[]\n",
    "    num_hashtags=[]\n",
    "    hashtags_ratio=[]\n",
    "    num_favorites=[]\n",
    "    num_impressions=[]\n",
    "    num_rank=[]\n",
    "    for file_name in file_list:\n",
    "        \n",
    "        with open(file_name,encoding='utf8') as file:\n",
    "        #Each line is a tweet info\n",
    "            data=file.readlines()\n",
    "\n",
    "        for line in data:\n",
    "            json_object=json.loads(line)\n",
    "            time_list.append(json_object['citation_date'])#extract time a tweet is posted by\n",
    "            num_followers.append(json_object['author']['followers'])#extract number of followers of teh person tweeting\n",
    "            num_retweets.append(json_object['metrics']['citations']['total'])#extract number of retweets of a tweet\n",
    "            num_replies.append(json_object['metrics']['citations']['replies'])#extract number of replies\n",
    "            num_mentions.append(len(json_object['tweet']['entities']['user_mentions']))#extract number of mentions (length of the list)\n",
    "            mentions_ratio.append(len(json_object['tweet']['entities']['user_mentions'])/len(json_object['tweet']['text']))#extract ration of mentions\n",
    "            num_urls.append(len(json_object['tweet']['entities']['urls']))#extract number of urls\n",
    "            urls_ratio.append(len(json_object['tweet']['entities']['urls'])/len(json_object['tweet']['text']))#extract ratio of urls\n",
    "            num_hashtags.append(len(json_object['tweet']['entities']['hashtags']))#extract number of hastages\n",
    "            hashtags_ratio.append(len(json_object['tweet']['entities']['hashtags'])/len(json_object['tweet']['text']))#extract ratio of hastages\n",
    "            num_favorites.append(json_object['tweet']['favorite_count'])\n",
    "            num_impressions.append(json_object['metrics']['impressions'])\n",
    "            num_rank.append(json_object['metrics']['ranking_score'])\n",
    "        file.close()\n",
    "    date_list=[]\n",
    "    pst_tz=pytz.timezone('America/Los_Angeles')#time info conversion\n",
    "    for time in time_list:\n",
    "        date=datetime.datetime.fromtimestamp(time,pst_tz).replace(tzinfo=None)\n",
    "        date_list.append(date)#save converted time into a new list\n",
    "    date_df=pd.DataFrame(date_list,columns=['Date'])#generate a dataframe based on date\n",
    "    date_df=date_df.set_index(date_df['Date'])#set a date string as index\n",
    "    #print(date_df)\n",
    "    date_df.drop(['Date'],'columns',inplace=True)#discard the second column\n",
    "    date_df['num_tweets']=1 #initialize a new column and set the column name as num_tweets\n",
    "    date_df['num_followers']=num_followers\n",
    "    date_df['num_retweets']=num_retweets\n",
    "    date_df['num_replies']=num_replies\n",
    "    date_df['num_mentions']=num_mentions\n",
    "    date_df['num_urls']=num_urls\n",
    "    date_df['num_hashtags']=num_hashtags\n",
    "    date_df['num_favorites']=num_favorites\n",
    "    date_df['num_impressions']=num_impressions\n",
    "    date_df['num_rank']=num_rank\n",
    "    \n",
    "    return date_df\n",
    "\n",
    "def readfile_HM(file_name):\n",
    "    with open(file_name,encoding='utf8') as file:\n",
    "        #Each line is a tweet info\n",
    "        data=file.readlines()\n",
    "    time_list=[]\n",
    "    num_followers=[]\n",
    "    num_retweets=[]\n",
    "    num_replies=[]\n",
    "    num_mentions=[]\n",
    "    mentions_ratio=[]\n",
    "    num_urls=[]\n",
    "    urls_ratio=[]\n",
    "    num_hashtags=[]\n",
    "    hashtags_ratio=[]\n",
    "    num_favorites=[]\n",
    "    num_impressions=[]\n",
    "    num_rank=[]\n",
    "    for line in data:\n",
    "        json_object=json.loads(line)\n",
    "        time_list.append(json_object['citation_date'])#extract time a tweet is posted by\n",
    "        num_followers.append(json_object['author']['followers'])#extract number of followers of teh person tweeting\n",
    "        num_retweets.append(json_object['metrics']['citations']['total'])#extract number of retweets of a tweet\n",
    "        num_replies.append(json_object['metrics']['citations']['replies'])#extract number of replies\n",
    "        num_mentions.append(len(json_object['tweet']['entities']['user_mentions']))#extract number of mentions (length of the list)\n",
    "        #mentions_ratio.append(len(json_object['tweet']['entities']['user_mentions'])/len(json_object['tweet']['text']))#extract ration of mentions\n",
    "        num_urls.append(len(json_object['tweet']['entities']['urls']))#extract number of urls\n",
    "        #urls_ratio.append(len(json_object['tweet']['entities']['urls'])/len(json_object['tweet']['text']))#extract ratio of urls\n",
    "        num_hashtags.append(len(json_object['tweet']['entities']['hashtags']))#extract number of hastages\n",
    "        #hashtags_ratio.append(len(json_object['tweet']['entities']['hashtags'])/len(json_object['tweet']['text']))#extract ratio of hastages\n",
    "        num_favorites.append(json_object['tweet']['favorite_count'])\n",
    "        num_impressions.append(json_object['metrics']['impressions'])\n",
    "        num_rank.append(json_object['metrics']['ranking_score'])\n",
    "    file.close()\n",
    "    date_list=[]\n",
    "    pst_tz=pytz.timezone('America/Los_Angeles')#time info conversion\n",
    "    for time in time_list:\n",
    "        date=datetime.datetime.fromtimestamp(time,pst_tz).replace(tzinfo=None)\n",
    "        date_list.append(date)#save converted time into a new list\n",
    "    date_df=pd.DataFrame(date_list,columns=['Date'])#generate a dataframe based on date\n",
    "    date_df=date_df.set_index(date_df['Date'])#set a date string as index\n",
    "    #print(date_df)\n",
    "    date_df.drop(['Date'],'columns',inplace=True)#discard the second column\n",
    "    date_df['num_tweets']=1 #initialize a new column and set the column name as num_tweets\n",
    "    date_df['num_followers']=num_followers\n",
    "    date_df['num_retweets']=num_retweets\n",
    "    date_df['num_replies']=num_replies\n",
    "    date_df['num_mentions']=num_mentions\n",
    "    date_df['num_urls']=num_urls\n",
    "    date_df['num_hashtags']=num_hashtags\n",
    "    date_df['num_favorites']=num_favorites\n",
    "    date_df['num_impressions']=num_impressions\n",
    "    date_df['num_rank']=num_rank\n",
    "    return date_df\n",
    "\n",
    "def feature_extraction(date_df,time_period):\n",
    "    feature_map=date_df.resample(time_period).sum()#total number of tweets, total num_followers, total num_retweets, replies, mentions, urls, hashtags\n",
    "    #date_df['mentions_ratio']=mentions_ratio\n",
    "    #date_df['urls_ratio']=urls_ratio\n",
    "    #date_df['hashtags_ratio']=hashtags_ratio\n",
    "    feature_map['max_followers']=date_df['num_followers'].resample(time_period).max()#pick maximum num of followers and resample\n",
    "    feature_map['max_replies']=date_df['num_replies'].resample(time_period).max()\n",
    "    feature_map['max_mentions']=date_df['num_mentions'].resample(time_period).max()\n",
    "    feature_map['max_urls']=date_df['num_urls'].resample(time_period).max()\n",
    "    feature_map['max_hashtags']=date_df['num_hashtags'].resample(time_period).max()\n",
    "    #feature_map['mentions_ratio']=date_df['mentions_ratio'].resample('H').mean()\n",
    "    #feature_map['urls_ratio']=date_df['urls_ratio'].resample('H').mean()\n",
    "    #feature_map['hashtags_ratio']=date_df['hashtags_ratio'].resample('H').mean()\n",
    "    feature_map['max_favorites']=date_df['num_favorites'].resample(time_period).max()\n",
    "    feature_map['max_impressions']=date_df['num_impressions'].resample(time_period).max()\n",
    "    feature_map['max_rank']=date_df['num_rank'].resample(time_period).max()\n",
    "    feature_map['time']=feature_map.index.hour#extract hour index from 0 to 23\n",
    "    return feature_map\n",
    "\n",
    "def divide_three(date_df):\n",
    "    start_time='2015-02-01 08:00:00'\n",
    "    end_time='2015-02-01 20:00:00'\n",
    "    #the first part\n",
    "    feature_map_first=feature_extraction(date_df,'H')\n",
    "    feature_map_first=feature_map_first.iloc[feature_map_first.index<start_time,:]\n",
    "    #the second part\n",
    "    feature_map_second=feature_extraction(date_df,'5T')\n",
    "    feature_map_second=feature_map_second.iloc[feature_map_second.index<=end_time,:]\n",
    "    feature_map_second=feature_map_second.iloc[feature_map_second.index>=start_time,:]\n",
    "    #the third part\n",
    "    feature_map_third=feature_extraction(date_df,'H')\n",
    "    feature_map_third=feature_map_third.iloc[feature_map_third.index>end_time,:]\n",
    "    \n",
    "    return feature_map_first, feature_map_second, feature_map_third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list=['tweets_#gohawks.txt','tweets_#gopatriots.txt','tweets_#nfl.txt','tweets_#patriots.txt','tweets_#sb49.txt','tweets_#superbowl.txt']\n",
    "date_df_agg=readfile_aggregate(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_map_all_first,feature_map_all_second,feature_map_all_third=divide_three(date_df_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the first time segment\n",
    "#s0_p1,s1_p1,s2_p1\n",
    "X_train=feature_map_all_first.iloc[:-1]\n",
    "Y_train=feature_map_all_first['num_tweets'][1:]\n",
    "X_train=np.nan_to_num(X_train)\n",
    "Y_train=np.nan_to_num(Y_train)\n",
    "scalar=StandardScaler()\n",
    "X_train_scale=scalar.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27916.621093240265\n",
      "[159.19841027 373.06134633 211.64989163 260.19709543 274.70698026\n",
      " 297.52282734]\n"
     ]
    }
   ],
   "source": [
    "date_df_s0_p1=readfile_HM('sample0_period1.txt')\n",
    "feature_s0_p1=feature_extraction(date_df_s0_p1,'H')\n",
    "Y_test=feature_s0_p1['num_tweets'][1:]\n",
    "feature_s0_p1=np.nan_to_num(feature_s0_p1)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=GradientBoostingRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=1,min_samples_split=10,n_estimators=1000)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s0_p1)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12961.697368590507\n",
      "[215.36516421 175.69361579 184.80403928 543.17412863 620.60666141\n",
      " 870.5883959 ]\n"
     ]
    }
   ],
   "source": [
    "date_df_s1_p1=readfile_HM('sample1_period1.txt')\n",
    "feature_s1_p1=feature_extraction(date_df_s1_p1,'H')\n",
    "Y_test=feature_s1_p1['num_tweets'][1:]\n",
    "feature_s1_p1=np.nan_to_num(feature_s1_p1)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=GradientBoostingRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=1,min_samples_split=10,n_estimators=1000)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s1_p1)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11614.880866328454\n",
      "[353.49830566 178.24384163 134.66626599 152.8285168  129.07648929\n",
      "  88.38401825]\n"
     ]
    }
   ],
   "source": [
    "date_df_s2_p1=readfile_HM('sample2_period1.txt')\n",
    "feature_s2_p1=feature_extraction(date_df_s2_p1,'H')\n",
    "Y_test=feature_s2_p1['num_tweets'][1:]\n",
    "feature_s2_p1=np.nan_to_num(feature_s2_p1)\n",
    "scalar=StandardScaler()\n",
    "feature_s2_p1_scale=scalar.fit_transform(feature_s2_p1)\n",
    "rf_mdl=GradientBoostingRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=1,min_samples_split=10,n_estimators=1000)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s2_p1)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the second time segment\n",
    "#s0_p2,s2_p2,s2_p2\n",
    "X_train=feature_map_all_second.iloc[:-1]\n",
    "Y_train=feature_map_all_second['num_tweets'][1:]\n",
    "X_train=np.nan_to_num(X_train)\n",
    "Y_train=np.nan_to_num(Y_train)\n",
    "scalar=StandardScaler()\n",
    "X_train_scale=scalar.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302087.5458595278\n",
      "[1781.21660692 2229.09451058 1949.30689729 2275.79838069 2106.91156367\n",
      " 2040.13959678]\n"
     ]
    }
   ],
   "source": [
    "date_df_s0_p2=readfile_HM('sample0_period2.txt')\n",
    "feature_s0_p2=feature_extraction(date_df_s0_p2,'5T')\n",
    "Y_test=feature_s0_p2['num_tweets'][1:]\n",
    "feature_s0_p2=np.nan_to_num(feature_s0_p2)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=RandomForestRegressor(max_depth=50,max_features='sqrt',min_samples_leaf=2,min_samples_split=2,n_estimators=1800)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s0_p2)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2099291.3707208107\n",
      "[2518.82275353 2502.29474559 2507.02886332 1737.15778329 2435.67223104\n",
      " 2215.61793871]\n"
     ]
    }
   ],
   "source": [
    "date_df_s1_p2=readfile_HM('sample1_period2.txt')\n",
    "feature_s1_p2=feature_extraction(date_df_s1_p2,'5T')\n",
    "Y_test=feature_s1_p2['num_tweets'][1:]\n",
    "feature_s1_p2=np.nan_to_num(feature_s1_p2)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=RandomForestRegressor(max_depth=50,max_features='sqrt',min_samples_leaf=2,min_samples_split=2,n_estimators=1800)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s1_p2)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2607849.013290526\n",
      "[1464.9920496  1755.58385538 1765.23672178 1702.23260141 1488.22509877\n",
      " 1510.5283772 ]\n"
     ]
    }
   ],
   "source": [
    "date_df_s2_p2=readfile_HM('sample2_period2.txt')\n",
    "feature_s2_p2=feature_extraction(date_df_s2_p2,'5T')\n",
    "Y_test=feature_s2_p2['num_tweets'][1:]\n",
    "feature_s2_p2=np.nan_to_num(feature_s2_p2)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=RandomForestRegressor(max_depth=50,max_features='sqrt',min_samples_leaf=2,min_samples_split=2,n_estimators=1800)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s2_p2)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For the third time segment\n",
    "#s0_p2,s2_p2,s2_p2\n",
    "X_train=feature_map_all_third.iloc[:-1]\n",
    "Y_train=feature_map_all_third['num_tweets'][1:]\n",
    "X_train=np.nan_to_num(X_train)\n",
    "Y_train=np.nan_to_num(Y_train)\n",
    "scalar=StandardScaler()\n",
    "X_train_scale=scalar.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305.6043929183226\n",
      "[ 28.63610658 100.88628185  57.59947799  53.25457925  67.42836577\n",
      " 144.49891697]\n"
     ]
    }
   ],
   "source": [
    "#For the third time segment\n",
    "#s0_p3,s1_p3,s2_p3\n",
    "date_df_s0_p3=readfile_HM('sample0_period3.txt')\n",
    "feature_s0_p3=feature_extraction(date_df_s0_p3,'H')\n",
    "Y_test=feature_s0_p3['num_tweets'][1:]\n",
    "feature_s0_p3=np.nan_to_num(feature_s0_p3)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=GradientBoostingRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=4,min_samples_split=5,n_estimators=400)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s0_p3)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15416.402362682824\n",
      "[354.61637145 -12.51105875  13.55221866  -2.05479516  37.04874762\n",
      "  19.55290619]\n"
     ]
    }
   ],
   "source": [
    "date_df_s1_p3=readfile_HM('sample1_period3.txt')\n",
    "feature_s1_p3=feature_extraction(date_df_s1_p3,'H')\n",
    "Y_test=feature_s1_p3['num_tweets'][1:]\n",
    "feature_s1_p3=np.nan_to_num(feature_s1_p3)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=GradientBoostingRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=4,min_samples_split=5,n_estimators=400)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s1_p3)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10237.412680761923\n",
      "[ 45.66088963  41.4898875   63.66547167 307.89828437  23.23292322\n",
      "  35.1644546 ]\n"
     ]
    }
   ],
   "source": [
    "date_df_s2_p3=readfile_HM('sample2_period3.txt')\n",
    "feature_s2_p3=feature_extraction(date_df_s2_p3,'H')\n",
    "Y_test=feature_s2_p3['num_tweets'][1:]\n",
    "feature_s2_p3=np.nan_to_num(feature_s2_p3)\n",
    "scalar=StandardScaler()\n",
    "rf_mdl=GradientBoostingRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=4,min_samples_split=5,n_estimators=400)\n",
    "rf_mdl.fit(X_train,Y_train)\n",
    "Y_pred=rf_mdl.predict(feature_s2_p3)\n",
    "MSE=mean_squared_error(Y_pred[:-1],Y_test)\n",
    "print(MSE)\n",
    "print(Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
